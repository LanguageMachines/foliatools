Usage: FoLiA-stats [options] file/dir

FoLiA-stats will produce ngram statistics for a FoLiA file, 
or a whole directory of FoLiA files 
The output will be a 2 or 4 columned tab separated file, extension: *tsv 
	 (4 columns when -p is specified)
	--clip='factor'	 clipping factor. 
		(entries with frequency <= 'factor' will be ignored). 
	-p	 output percentages too. 
	--lower	 Lowercase all words
	--separator='sep' 	connect all n-grams with 'sep' (default is an underscore)
	--underscore	 Obsolete. Equals to --separator='_'
	--languages	 Lan1,Lan2,Lan3. (default='Lan1').
		 languages that are not assigned to Lan1,Lan2,... are counted as Lan1
		 If Lan1='skip' all languages not mentioned as Lan2,... are ignored.
		 If Lan1='all' all detected languages are counted.
	--lang='lan' (default='none').
		 This is a shorthand for --languages=skip,lan.
		 The value 'none' (the default) means: accept all languages.
		 Meaning: only count words from 'lan' ignoring all other languages.
	--ngram='count'		 construct n-grams of length 'count'
	--max-ngram='max'	 construct ALL n-grams upto a length of 'max'
		 If --ngram='min' is specified too, ALL n-grams from 'min' upto 'max' are created
	--mode='mode' Special actions:
		 'string_in_doc' Collect ALL <str> nodes from the document and handle them 
			as one long Sentence.
		 'word_in_doc' Collect ALL <w> nodes from the document and handle them 
			as one long Sentence.
		 'lemma_pos' When processsing nodes, also collect lemma and POS tag information. 
			 this implies --tags=s
	--tags='tags' collect text from all nodes in the list 'tags'
	--skiptags='tags' skip all nodes in the list 'tags'
	-s	 obsolete, equals --tags=p
	-S	 obsolete, equals --mode=string_in_doc
	--class='name'	 When processing <str> nodes, use 'name' as the folia class for <t> nodes.
		 (default is 'current')
	--collect	 collect all n-gram values in one file.
	--aggregate create per n-gram a combined frequency table list for ALL languages detected.
	--hemp=<file>	 Create a historical emphasis file. 
		 (words consisting of single, space separated letters)
	--detokenize when processing FoLiA with ucto tokenizer info, UNDO that tokenization. (default is to keep it)
	-t <threads>
	--threads <threads> Number of threads to run on.
			 If 'threads' has the value "max", the number of threads is set to a
			 reasonable value. (OMP_NUM_TREADS - 2)
	-h or --help	 this message
	-v or --verbose	 very verbose output.
	-V or --version	 show version 
	-e	 expr: specify the expression all input files should match with.
	-o	 name of the output file(s) prefix.
	-R	 search the dirs recursively (when appropriate).
FoLiA-stats from foliautils 0.22
Usage: FoLiA-stats [options] file/dir

FoLiA-stats will produce ngram statistics for a FoLiA file, 
or a whole directory of FoLiA files 
The output will be a 2 or 4 columned tab separated file, extension: *tsv 
	 (4 columns when -p is specified)
	--clip='factor'	 clipping factor. 
		(entries with frequency <= 'factor' will be ignored). 
	-p	 output percentages too. 
	--lower	 Lowercase all words
	--separator='sep' 	connect all n-grams with 'sep' (default is an underscore)
	--underscore	 Obsolete. Equals to --separator='_'
	--languages	 Lan1,Lan2,Lan3. (default='Lan1').
		 languages that are not assigned to Lan1,Lan2,... are counted as Lan1
		 If Lan1='skip' all languages not mentioned as Lan2,... are ignored.
		 If Lan1='all' all detected languages are counted.
	--lang='lan' (default='none').
		 This is a shorthand for --languages=skip,lan.
		 The value 'none' (the default) means: accept all languages.
		 Meaning: only count words from 'lan' ignoring all other languages.
	--ngram='count'		 construct n-grams of length 'count'
	--max-ngram='max'	 construct ALL n-grams upto a length of 'max'
		 If --ngram='min' is specified too, ALL n-grams from 'min' upto 'max' are created
	--mode='mode' Special actions:
		 'string_in_doc' Collect ALL <str> nodes from the document and handle them 
			as one long Sentence.
		 'word_in_doc' Collect ALL <w> nodes from the document and handle them 
			as one long Sentence.
		 'lemma_pos' When processsing nodes, also collect lemma and POS tag information. 
			 this implies --tags=s
	--tags='tags' collect text from all nodes in the list 'tags'
	--skiptags='tags' skip all nodes in the list 'tags'
	-s	 obsolete, equals --tags=p
	-S	 obsolete, equals --mode=string_in_doc
	--class='name'	 When processing <str> nodes, use 'name' as the folia class for <t> nodes.
		 (default is 'current')
	--collect	 collect all n-gram values in one file.
	--aggregate create per n-gram a combined frequency table list for ALL languages detected.
	--hemp=<file>	 Create a historical emphasis file. 
		 (words consisting of single, space separated letters)
	--detokenize when processing FoLiA with ucto tokenizer info, UNDO that tokenization. (default is to keep it)
	-t <threads>
	--threads <threads> Number of threads to run on.
			 If 'threads' has the value "max", the number of threads is set to a
			 reasonable value. (OMP_NUM_TREADS - 2)
	-h or --help	 this message
	-v or --verbose	 very verbose output.
	-V or --version	 show version 
	-e	 expr: specify the expression all input files should match with.
	-o	 name of the output file(s) prefix.
	-R	 search the dirs recursively (when appropriate).
test data: [aap,N,A,P,noot]
inventory: [NO HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NO HEMP]
HEMPS: {N_A_P}
test data: [aap,N,A,P.,noot]
inventory: [NO HEMP,NORMAL HEMP,NORMAL HEMP,END PUNCT HEMP,NO HEMP]
HEMPS: {N_A_P.}
test data: [aap,N,(A,P),noot]
inventory: [NO HEMP,NO HEMP,START PUNCT HEMP,END PUNCT HEMP,NO HEMP]
HEMPS: {(A_P)}
test data: [aap,N.,A,P,noot]
inventory: [NO HEMP,NO HEMP,NORMAL HEMP,NORMAL HEMP,NO HEMP]
HEMPS: {A_P}
test data: [aap,N,A.,P.,noot]
inventory: [NO HEMP,NORMAL HEMP,END PUNCT HEMP,NO HEMP,NO HEMP]
HEMPS: {N_A.}
test data: [aap,(N,(A,P),noot]
inventory: [NO HEMP,NO HEMP,START PUNCT HEMP,END PUNCT HEMP,NO HEMP]
HEMPS: {(A_P)}
test data: [aap,(N,(A,.P,noot]
inventory: [NO HEMP,NO HEMP,NO HEMP,NO HEMP,NO HEMP]
HEMPS: {}
test data: [1,2,van,J.,Brouwer]
inventory: [NORMAL HEMP,NORMAL HEMP,NO HEMP,NO HEMP,NO HEMP]
HEMPS: {1_2}
test data: [1,2,van,J.]
inventory: [NORMAL HEMP,NORMAL HEMP,NO HEMP,NO HEMP]
HEMPS: {1_2}
test data: [diamanthandel.,8]
inventory: [NO HEMP,NO HEMP]
HEMPS: {}
test data: [een,F,1,o,r,e,n,t,ij,n,e,r,,is]
inventory: [NO HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,NORMAL HEMP,END PUNCT HEMP,NO HEMP]
HEMPS: {F_1_o_r_e_n_t_ij_n_e_r,}
start processing of 2 files 
processing using prefix: stats8.
Processed :data/frog.xml with 11 n-grams, 11 lemmas, and 11 POS tags. still 1 files to go.
Processed :data/frog2.xml with 11 n-grams, 11 lemmas, and 11 POS tags. still 0 files to go.
done processsing with prefix 'stats8.'
start calculating the results
in total 22 n-grams were found.in 2 FoLiA documents.
created WordFreq list 'stats8.wordfreqlist.tsv' for 1-grams. Stored 22 tokens and 8 types, TTR= 0.363636, the angle is 19.9831 degrees
created LemmaFreq list 'stats8.lemmafreqlist.tsv' for 1-gram lemmas. Stored 22 tokens and 7 types. TTR= 0.318182, the angle is 17.6501 degrees
created LemmaPosFreq list 'stats8.lemmaposfreqlist.tsv' for 1-gram Lemma-Pos pairs. Stored 22 tokens and 7 types. TTR= 0.318182, the angle is 17.6501 degrees
start processing of 1 files 
processing using prefix: stats12.
Processed :data/sonar.xml with 998 n-grams, 998 lemmas, and 998 POS tags. still 0 files to go.
done processsing with prefix 'stats12.'
start calculating the results
in total 998 n-grams were found.
created WordFreq list 'stats12.wordfreqlist.tsv' for 1-grams. Stored 274 tokens and 153 types, TTR= 0.558394, the angle is 29.1787 degrees
created WordFreq list 'stats12.wordfreqlist.2-gram.tsv' for 2-grams. Stored 257 tokens and 241 types, TTR= 0.937743, the angle is 43.1598 degrees
created WordFreq list 'stats12.wordfreqlist.3-gram.tsv' for 3-grams. Stored 241 tokens and 239 types, TTR= 0.991701, the angle is 44.7613 degrees
created WordFreq list 'stats12.wordfreqlist.4-gram.tsv' for 4-grams. Stored 226 tokens and 225 types, TTR= 0.995575, the angle is 44.873 degrees
created LemmaFreq list 'stats12.lemmafreqlist.tsv' for 1-gram lemmas. Stored 274 tokens and 137 types. TTR= 0.5, the angle is 26.5651 degrees
created LemmaFreq list 'stats12.lemmafreqlist.2-gram.tsv' for 2-gram lemmas. Stored 257 tokens and 238 types. TTR= 0.92607, the angle is 42.8018 degrees
created LemmaFreq list 'stats12.lemmafreqlist.3-gram.tsv' for 3-gram lemmas. Stored 241 tokens and 238 types. TTR= 0.987552, the angle is 44.6412 degrees
created LemmaFreq list 'stats12.lemmafreqlist.4-gram.tsv' for 4-gram lemmas. Stored 226 tokens and 225 types. TTR= 0.995575, the angle is 44.873 degrees
created LemmaPosFreq list 'stats12.lemmaposfreqlist.tsv' for 1-gram Lemma-Pos pairs. Stored 274 tokens and 151 types. TTR= 0.551095, the angle is 28.8589 degrees
created LemmaPosFreq list 'stats12.lemmaposfreqlist.2-gram.tsv' for 2-gram Lemma-Pos pairs. Stored 257 tokens and 239 types. TTR= 0.929961, the angle is 42.9216 degrees
created LemmaPosFreq list 'stats12.lemmaposfreqlist.3-gram.tsv' for 3-gram Lemma-Pos pairs. Stored 241 tokens and 239 types. TTR= 0.991701, the angle is 44.7613 degrees
created LemmaPosFreq list 'stats12.lemmaposfreqlist.4-gram.tsv' for 4-gram Lemma-Pos pairs. Stored 226 tokens and 225 types. TTR= 0.995575, the angle is 44.873 degrees
